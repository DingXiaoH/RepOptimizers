# RepOptimizers

This is the official repository of [Re-parameterizing Your Optimizers rather than Architectures](https://arxiv.org/abs/2205.15242).

If you find the paper or this repository helpful, please consider citing

        @article{ding2022re,
        title={Re-parameterizing Your Optimizers rather than Architectures},
        author={Ding, Xiaohan and Chen, Honghao and Zhang, Xiangyu and Huang, Kaiqi and Han, Jungong and Ding, Guiguang},
        journal={arXiv preprint arXiv:2205.15242},
        year={2022}
        }

## Catalog
- [x] Model code
- [] PyTorch pretrained models
- [] PyTorch training code

<!-- ✅ ⬜️  -->

## Pre-trained Models

| name | resolution |ImageNet-1K acc | #params | FLOPs | ImageNet-1K pretrained model |
|:---:|:---:|:---:|:---:| :---:|:---:|
|RepLKNet-31B|224x224|83.5| 79M   |  15.3G   |[Google Drive](https://drive.google.com/file/d/1azQUiCxK9feYVkkrPqwVPBtNsTzDrX7S/view?usp=sharing), [Baidu](https://pan.baidu.com/s/1gspbbfqooMtegt_DO1TUeA?pwd=lknt)|
|RepLKNet-31B|384x384|84.8| 79M   |  45.1G   |[Google Drive](https://drive.google.com/file/d/1vo-P3XB6mRLUeDzmgv90dOu73uCeLfZN/view?usp=sharing), [Baidu](https://pan.baidu.com/s/1WhLaCKKv4NuKc3qMYECOIQ?pwd=lknt)|


## Evaluation


## Training

### Hyper-Search on CIFAR-100

### Train the target model on ImageNet


## License
This project is released under the MIT license. Please see the [LICENSE](LICENSE) file for more information.

